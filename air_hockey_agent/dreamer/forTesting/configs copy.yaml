defaults:
  # Logging and General Configuration
  logdir: null
  traindir: null
  evaldir: null
  seed: 0
  deterministic_run: False
  steps: 1e6
  eval_every: 1e4
  eval_episode_num: 10
  log_every: 1e4
  reset_every: 0
  device: 'cuda:0'
  compile: True
  precision: 32
  debug: False

  # Environment
  task: 'airhockey_hit'
  envs: 1
  action_repeat: 2
  time_limit: 1000
  prefill: 2500
  size: null  # Not relevant since your observations are numerical data

  # Model (reduced for your use case)
  dyn_hidden: 128  # Reduced for simplicity
  dyn_deter: 128
  dyn_stoch: 16
  dyn_discrete: 0  # If no categorical latent space is required
  units: 256
  act: 'ReLU'
  norm: True
  encoder: {mlp_keys: '.*', cnn_keys: '$^', act: 'ReLU', norm: True, mlp_layers: 2, mlp_units: 256}
  decoder: {mlp_keys: '.*', cnn_keys: '$^', act: 'ReLU', norm: True, mlp_layers: 2, mlp_units: 256}

  actor:
    layers: 2
    dist: 'normal'
    entropy: 1e-3
    std: 'learned'
    min_std: 0.1
    max_std: 1.0
    lr: 1e-4
    eps: 1e-5
    grad_clip: 100.0
  critic:
    layers: 2
    lr: 1e-4
    eps: 1e-5
    grad_clip: 100.0

  reward_head:
    layers: 2
    loss_scale: 1.0
  cont_head:
    layers: 2
    loss_scale: 1.0

  # Training
  batch_size: 32
  batch_length: 50
  train_ratio: 256
  pretrain: 100
  model_lr: 1e-4
  opt_eps: 1e-8
  grad_clip: 1000
  opt: 'adam'

  # Behavior
  discount: 0.99
  discount_lambda: 0.95
  imag_horizon: 10
  eval_state_mean: False

  # Exploration
  expl_behavior: 'epsilon_greedy'
  expl_until: 10000
  expl_extr_scale: 0.0
  expl_intr_scale: 1.0